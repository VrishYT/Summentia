[
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_1.png",
		"transcripts": [
			" In this video, we'll talk about gradient descent and some of the important things we need to consider when optimizing our multi-layer neural networks."
		],
		"summaries": [],
		"squashed": false
	},
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_2.png",
		"transcripts": [
			" We optimize and train our neural models using gradient descent and this means repeatedly updating our parameters by taking small steps in the negative direction of the partial derivative. So at each iteration the model slowly gets better and better and moves towards a lower value of the loss function. You can imagine this as standing on a large surface with different hills and valleys and you want to get to the lowest point on that surface. The problem is that you can't see anything. All you can do is you can feel the ground at the point where you're at at the given time. So you can feel the slope which way the ground is sloping and this can be your best guess towards which direction you should be going in in order to get to a lower point. So you take a small step in the direction that is sloping down and then you reevaluate. You feel which way the ground is sloping. Now maybe it's sloping in a different direction so you take the next step in a different direction. And you can see that in the graph over here of how we take small steps. We start somewhere up here but after taking many gradient descent steps we get to a lower point here or depending on where we started we might end up in a different minimum instead. So here is the general very important formula for updating our weights during gradient descent training. So it's the new weights equal the old weights minus the learning grade times the partial derivative of the loss with respect to the weights. So the learning grade is just a hyper parameter that we choose and we can tune this for example on the development data. So we train many different models with different learning grades and we see which one performs best on the development set. Now in order to do gradient descent we have to construct our neural networks in a way that it can be back propagated through. So all the functions that we are using in our neural network they have to be differentiable and the loss needs to be differentiable as well. So all of the functions, activation functions, loss functions that we've covered except for the perceptron. These are all differentiable so you can use them in your neural networks but if you come up with your own activation functions or your own loss functions or your own customer neural network architectures then you do need to make sure that they are differentiable and you are calculating gradients through them correctly."
		],
		"summaries": [],
		"squashed": false
	},
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_5.png",
		"transcripts": [
			" Here is the general algorithm for how we actually apply gradient descent in practice to update the parameters of our neural network. So first we need to somehow initialize our weights and we'll talk about weight initialization in a few slides. But in general we want to initialize them randomly. We then have a loop that add each step. It calculates the gradient based on the whole dataset and then updates the weights using that gradient, using the gradient descent step rule. And then we finish when we're done, when the loss function doesn't improve anymore. Well, first thing to note is that you should be computing your gradients before you're updating the weights. So make sure that all the gradients in your network are calculated before you actually apply the weights because otherwise you might end up with some errors because the weights themselves are part of the back propagation calculation. You want to make sure that you're not updating the weights with new values before you actually finished using those weights to do the back propagation. Because otherwise you will end up using the new weights for back propagation. This doesn't match up with the loss function anymore. So you're just going to end up with incorrect values and errors in your network. Another thing to note is that applying gradient descent like this is usually inefficient. We don't normally do this unless the dataset is very small. So there's two reasons for this. In practice, many large datasets are too big to calculate the gradient over all of it at the same time. So it would be easier if we can break this up into smaller parts. And the second reason is that it's not very efficient because we have to wait until we've processed the whole dataset until we do one small step of gradient descent. So it's going to take a long time until we can just do one little update to our parameters. At the other end of the extreme, we have stochastic gradient descent. And that means we have a loop over all of the data points. And we calculate the gradient based only on one data point and then immediately update the weights based on that data point. That means we get really fast updates all the time after processing just one data point. But the downside is that this is very noisy. There's only limited amounts of information in a single data point. So we'll end up taking gradient descent steps that roughly will take us in the correct direction, but there's going to be a lot of noise involved. So we'll wiggle around and try to look for the correct solution a lot of the time. So what we actually do is mini batch gradient descent. This is kind of a balance between the two approaches. And there we have a loop over batches of data points. So at each pass through our data, we would normally shuffle the training data and then separate it into batches of whatever size we want. And then go through each of these batches, calculate the gradient based on that batch, and then update the weights based on that, based on the gradient of that batch. So that kind of separates the data set into smaller chunks. We get faster updates, but each batch has this kind of average gradient from many different data points. So our direction in which we are taking our updates during gradient descent is more clear. It has less noise because the noise gets averaged out over the whole batch. Now the kinds of nice convex loss surfaces that we looked at with previous functions, these don't really apply to neural networks anymore. So neural networks have very complex loss surfaces and optimizing them can be quite difficult. So here you can see an example of trying to visualize a more realistic neural network loss surface. And when we are optimizing our neural network, then the task is basically to try to find the lowest point on that surface and imagine just being dropped to one random point on the surface, not being able to see anywhere around you and just based on the slope of the ground at the point where you're at, you need to find the lowest point. So there's quite a few difficulties in finding this point. But there are also methods that we can use to make this optimization task easier."
		],
		"summaries": [],
		"squashed": false
	},
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_6.png",
		"transcripts": [
			" One important thing to consider is the learning rate in our gradient descent update. So the size of the learning rate really matters for our optimization and how well we are able to optimize the model. If the learning rate is too low, as you can see on the animation on the left, then it's likely that we will get to a good minimum at the end, but the optimization can just take a really long time. It could take longer than we are willing to wait. On the other hand, if our learning rate is too high, then we can just step over the correct solution. So in this case, because as we're getting further from the optimum, the gradient is actually getting bigger, that means each time we're taking an even bigger step, trying to get to the minimum, but we end up stepping even further and therefore bouncing further and further away from the correct solution. And the middle animation shows what a good learning rate looks like, so we get to the minimum of the loss function and we get there in a reasonable number of steps. And the learning rate is just a hyperparameter that we have to choose and it's a good idea to choose that based on the development set."
		],
		"summaries": [],
		"squashed": false
	},
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_7.png",
		"transcripts": [
			" Adaptive learning grades are also something that can help us. So the idea behind adaptive learning grades is that instead of setting one learning grade for the whole model, we have different learning grades for each of the parameters in our model. And then there is a logic behind the adaptive learning grade that controls the size of each learning grade for each parameter. So if a parameter has not been updated for a while, then we might want to increase the learning rate for that parameter to see if maybe it has gotten stuck in a suboptimal state. At the same time, if a parameter is bouncing around rapidly between different iterations and making really big updates, then we might want to decrease the learning rate for that parameter to calm it down and allow it to find the minimum point in that dimension. And you can see an animation of that here. So these are just some of the adaptive learning grades that are available. And this is a particular difficult optimization problem. So this is a saddle point problem because the point where these parameters are released. It is a minimum in one dimension, but not a minimum in another dimension. So you can see that different algorithms, different adaptive learning rates take different amounts of time to escape that saddle point. And some of them don't escape at all. So the vanilla stochastic gradient descent stays stuck in the saddle point. In general, I don't recommend implementing your own adaptive learning rates, at least not when you're starting out. Different adaptive learning rates are implemented in most of the neural network libraries, like PyTorch. So you are able to just use their implementation included in your code with one or two lines of extra code. And just as a tip, I have found that Adam and Ada Delta are adaptive learning rates that work quite well for a very wide range of different tasks and data sets."
		],
		"summaries": [],
		"squashed": false
	},
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_8.png",
		"transcripts": [
			" A simple version of an adaptive learning grade that you can implement yourself is learning grade decay. And learning grade decay just means that you are scaling your learning grade by some value between 0 and 1, so the new learning grade will become smaller than it was before. So here we have an update rule for the learning grade that multiplies the original learning grade with this scalar. And the intuition here is that as you get closer to your correct answer, you want to be making smaller and smaller steps so that we don't overstep the correct answer. So we can decrease our learning grade after we have been training for some time. And there are different strategies for updating the learning grade. We can do this at every epoch. We can do this after we have been training for a few epochs already. Or we can start doing this if the performance on the validation set hasn't improved for a certain number of epochs, for example. So this is something that you can implement yourself for recently and you can try out it's effect, for example, in the coursework as well."
		],
		"summaries": [],
		"squashed": false
	},
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_9.png",
		"transcripts": [
			" Let's talk about weight initialization as well. In order to actually iteratively update our weights using gradient descent, we do need to set them to some value to begin with. So what kind of value should we use? The simplest approach to weight initialization would be to set them all to zero. And this is something that we use for biases. So we set the biases to zero because it makes sense for the network to not have any biases before it has started training. But we don't want to set all our weights to zero. Because if you look at the equations of the forward backward path of a neural network, then you'll see that if all our weights are the same, then many of the neurons will learn exactly the same thing. They will receive the same kinds of partial derivatives during back propagation. So they will be optimized to have the same values as well. And this is not what we want. So instead we want to initialize them randomly so that they all start out from different positions and each of them will be incentivized to learn different information from the same data set. So one option of random initialization would just be to draw them from a normal distribution. We commonly use mean zero and variance of either one or 0.1. And this gives us small values around zero to initialize with. Another possible strategy is a Javier Gloro. So this is an initialization strategy that's named after one of the authors. And the formula for that is here. So we draw values from a uniform distribution where the boundaries of the distribution are defined like this. And the nj stands for the number of neurons in the current layer. And nj plus 1 is the number of neurons in the next layer. So we are trying to initialize the weights between these two layers. And what this formula is doing is that if we have many neurons in both of these layers, then we will be drawing smaller and smaller values closer to zero. Whereas if we have very few neurons, then we will be initializing with larger weights as well."
		],
		"summaries": [],
		"squashed": false
	},
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_10.png",
		"transcripts": [
			" It's important to note that randomness plays quite an important role in neural networks. And that's because different random initializations actually lead to different results in the different performance. So you can see the figures here that we have exactly the same loss function in both cases, but because we start from a different location on that loss function, we end up optimizing into a different minimum and therefore we will have a slightly different results at the end as well. So this is something you need to consider. One solution to that would be to explicitly set the random seed. You would have to make sure to set all the random seeds because if you write a normal machine learning experiment, then there will be quite a few different random number generators that are used. So this is NumPy, PyTorch, maybe SidePy, the inbuilt Python random number generator. So all of these would have to be set and controlled. But even when you do this, you might not be able to control all of the randomness in the model because the way that deep learning libraries are implemented. In some implementations and in some functions, when the processes get parallelized to different threads on GPUs, they still end up with randomly different results. That's because different threads end in different times. And when combining the results back together, then there might be differences due to just pure randomness. And this doesn't affect all the operations, but it does happen fairly frequently. So the solution there is to just embrace the randomness. You can run your model with different random seeds. For example, five or ten different configurations. And then when you report your performance, you report the mean and the standard deviation of your performance. So this gives a full picture of what kind of results your model gets under different random initializations. We can see what the mean is and also how much the results can vary just due to randomness."
		],
		"summaries": [],
		"squashed": false
	},
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_11.png",
		"transcripts": [
			" something that often helps when optimizing neural network models is normalizing our input data. So there's two types of normalization that are quite often used. One is minmax normalization. And there we scale each of the features to be in a particular range. For example, 0 to 1 or minus 1 to 1. And the other is standardization, or it's also called Z normalization. And there we're scaling the data to have a mean zero and standard deviation of 1. And these are the equations you can use to normalize your data. Normalization helps because the weight updates are proportional to the input. As you can see in the equation here for the partial derivative with respect to a weight matrix. So we'll be taking different sized steps in different dimensions because of this. And that also means we might not be able to find a learning rate that works well for all of these dimensions. So if we can get them all the features to be in a similar range and also similar to the weights in our neural network. And then this usually helps the model learn more accurate models. Couple of things to remember. One is that you need to perform the normalization for each volume in your input data separately. So you don't normalize the whole matrix as a whole, but you just normalize one particular feature for all the data points. And two, you need to be calculating any normalizing constants based only on the training set. So for example, things like the X min, X max, mean and standard deviation. These you need to calculate based on the training set. You then apply the normalization on the training set. And then once it's time to apply the normalization also on the test set or the validation set. Then you use the same constants that you calculated on the training set. You don't calculate new ones because otherwise you might end up normalizing your test data differently than your training data. And that would not lead to good results."
		],
		"summaries": [],
		"squashed": false
	},
	{
		"slide": "/root/data/d97f4b74-52c1-4a1c-8f20-4368967c2631/slides/_page_12.png",
		"transcripts": [
			" Finally, I'll tell you about a useful method that allows us to check whether our gradient is actually calculated correctly in our implementation. And it's called gradient checking and it involves calculating the gradient in two different ways and then measuring that they are actually similar. So method one involves checking the gradient using the weight difference before and after you perform your gradient descent. So we just used the gradient descent formula to find this partial derivative for one particular weight. So we take the weight before you updated the weight using gradient descent and after. We take the difference and we divide by the learning rate and by the original update formula, this gives us the partial derivative of the loss with respect to that particular weight. Now method two is a more direct approach and we go back to the definition of the partial derivative. So here what we do is we take the original network, we modify one of the weights a little bit. So for example, we add a small constant epsilon and we measure the loss of the network at the very top. So this is the loss here. We then go back to the original network and we change the weight in the opposite direction. So now we subtract the epsilon and again we measure the loss at the very output. We take the difference of these two and divide by two epsilon and by the definition of the partial derivative. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one. And then we go back to the next one."
		],
		"summaries": [],
		"squashed": false
	}
]
